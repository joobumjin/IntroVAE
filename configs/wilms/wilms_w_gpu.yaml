opt:
  CUDA_VISIBLE_DEVICES: 0
  hdim: 512
  output_height: 256
  channels: [32, 64, 128, 256, 512, 512]
  m_plus: 120
  weight_rec: 0.05
  weight_kl: 1.0
  weight_neg: 0.5
  num_vae: 0
  trainsize: 29000
  test_iter: 1000 
  save_iter: 1 
  start_epoch: 0  
  batchSize: 16 
  nrow: 8 
  lr_e: 0.0002 
  lr_g: 0.0002  
  nEpochs: 500   
  #2>&1   | tee train0.log 
  dataset: wilms
  dataroot: ~/awesomePhD/Datasets/

  net_E_params:
    cdim: 3
    hdim: 512
    channels: [64, 128, 256, 512, 512, 512]
    image_size: 256
  
  net_D_params:
    cdim: 3
    hdim: 512
    channels: [64, 128, 256, 512, 512, 512]
    image_size: 256


trainer_opt:
  gpus: [0]
  max_steps: 200000
  precision: 32
  max_epochs: 16400
  min_epochs: 1
  log_save_interval: 1000
  num_sanity_val_steps: 1
  early_stop_callback: False
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  deterministic: True 
  #distributed_backend: dp

logging_opt:
  save_dir: ./logs/
  name: wilms
  version: wilms_w

ckpt_opt:
  save_top_k: -1
  period: 1000
